# PCA
The mathematical foundation of PCA (Principal Component Analysis)  

本部分内容，是基于在学习机器学习过程中，在无监督学习部分，PCA碰到的数学基础问题的解释  
此文件只是个人的心得，具体的相关数学知识，请前往官方网站或者查阅相关书籍进行查阅。  

# 方差、协方差、协方差矩阵

首先需要确定的是，当我们在处理数据的时候，通常使用向量、矩阵的形式，一个数据点为一个向量，向量的大小为该数据拥有的特征数量。  

## 方差：  
方差是数学中的基础概念，他解释了一组数据的离散程度  
公式：σ² = Σ(xᵢ - μ)² / N 或者 s² = Σ(xᵢ - x̄)² / (n - 1)  
前面为总体方差 后者为样本方差  
样本方差的分母是（n-1）相比较于总体方差分母（n）  
这样做的原因是因为，可以使样本方差成为总体方差的无偏估计。  
我们经常说，方差越大，数据拥有的信息就越大，这如何理解？  
先做一个极端的假设，当由一组数据的方差为0时，他能说明什么？它只能说明，这组数据的值全部相同，放在数据集中，每个样本点，在这个特征上没有任何的区别。  
以此类推，当一组数据的方差越大，就说明，数据的离散程度越大，放在数据集中，就说明每个数据点的这组特征，各有特点，可以更好地对这些数据点进行区别。  

### 无偏估计    
这里先不解释，为什么两种方差中的分母不同的原因  
“偏”：指的是一个估计量系统地高估或低估了它要估计的总体参数的趋势  
“无偏”：意味着这个估计量没有系统性的误差。长期来看，多次抽样估计的结果的平均值会等于真实的总体参数。  

### 标准差  
标准差就是方差的开平方

## 协方差：    
协方差和方差的区别在于，协方差比较的是，两组特征数据的关系，衡量的是两个变量之间的协同变换关系  
简单来说，当一个变量偏离它的平均值时，另一个变量是否也倾向于某种方向来偏离它的值？  
协方差同样分为，总体协方差和样本协方差  
总体协方差：Cov(X, Y) = Σ[(Xᵢ - μₓ) * (Yᵢ - μᵧ)] / N  
样本协方差：Cov(X, Y) = s_{xy} = Σ[(Xᵢ - X̄) * (Yᵢ - Ȳ)] / (n - 1)  
这里需要明白，对于计算出来的斜方差，它的数值大小没有太多实际意义，更多观察的是他的结果与0之间的关系，从而判断是正相关还是负相关。  
若想知道两组特征的关系，需要知道他们的相关系数。  

### 相关系数  
相关系数 (ρ 或 r) = Cov(X, Y) / (σₓ * σᵧ)  
注意公式后面，是两组数据方差的开平方，就是他们的标准差  
相关系数会在（-1，1）之间变动 1是完全正相关 -1是完全负相关 0无线性相关  

## 协方差矩阵：    
抽象的来说，协方差矩阵是一个“所有变量与所有其他变量的协方差”的总结表  
特点为：主对角线上的数据为各个特征的方差，其他位置，是不同特征间的协方差，协方差矩阵为对称矩阵  
协方差矩阵具有半正定性，即这个矩阵的特征值都大于或者等于0  
协方差矩阵的计算步骤：1中心化，每个样本减去其平均值 2构造矩阵 去中心化的数据矩阵 3矩阵乘法 样本协方差矩阵s S=1/（n-1）*A**T*A  







# 对PCA方法的工作流程 
## 第一步：数据标准化：    
相当于给每个特征一个“公平”，让它们在计算上站在同一个起跑线上  
<img width="854" height="387" alt="image" src="https://github.com/user-attachments/assets/af5f327b-85e9-4d56-a8b9-9872ffa32e5b" />

## 第二步：计算协方差矩阵：    
<img width="829" height="398" alt="image" src="https://github.com/user-attachments/assets/8c0426cc-2c0c-4537-a7d8-5a5e923627b2" />

## 第三步：计算协方差矩阵的特征值和特征向量：   
为什么协方差矩阵的特征值会和方差相关 这是因为求解的原因吗？  
是的，这完全是由我们求解主成分的过程所定义和推导出来的。特征值本质上就是数据在对应特征向量（主成分）方向上的方差。  
<img width="846" height="413" alt="image" src="https://github.com/user-attachments/assets/3398ff41-bbfa-40f7-ba7e-dd540eb46cda" />

## 第四步：对特征值排序并选择主成分：  
<img width="818" height="406" alt="image" src="https://github.com/user-attachments/assets/92ff82b1-25db-4502-b167-e3cabf0b017b" />

## 第五步：将数据投影到新的特征子空间：  
<img width="769" height="274" alt="image" src="https://github.com/user-attachments/assets/a5f66fea-1235-4b62-a57d-12dfd9499082" />

## 对于矩阵的物理空间理解：  
首先从简单的二维矩阵进行讨论  
矩阵看成两个在二维空间中的方向指针，它们构成一个平面  
三维矩阵中  
由三个向量构成一个空间  
依次类推 n维矩阵 则构成了一个n维空间  
以上的情况，需要满足每个向量互不线性相关  
假若其中的某个向量可以由其他向量表示，则表示的空间小于n维  
需要随时清楚，PCA是在无监督学习下的，所以请不要把分类等监督任务交给他，单纯的进行降维就可以了。

## 关于PCA降维和图像特征处理的一些问题  
我将与DEEPSEEK的聊天记录进行展示  
https://chat.deepseek.com/share/620oakl0y3yyydizvl
